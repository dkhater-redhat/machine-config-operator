I0120 21:05:26.387104       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:21:37.294938       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:21:37.294966       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.294974       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.294979       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.294984       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.294988       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.294991       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.294996       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.295000       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.295004       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.295007       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:21:37.295012       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:21:37.295016       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:21:37.295038       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:21:37.312264       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:21:37.312283       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312288       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312293       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.312297       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312301       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312306       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.312309       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312315       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:37.312318       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:21:37.312322       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
E0120 21:21:37.428784       1 render_controller.go:445] Error syncing Generated MCFG: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
E0120 21:21:37.434586       1 render_controller.go:467] Error updating MachineConfigPool worker: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
I0120 21:21:37.434607       1 render_controller.go:382] Error syncing machineconfigpool worker: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
I0120 21:21:42.329711       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:21:42.329735       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329743       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329748       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.329753       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329757       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329761       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.329765       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329769       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.329772       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.329776       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:21:42.329781       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:21:42.329785       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:21:42.329793       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:21:42.346473       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:21:42.346494       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346501       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346506       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.346510       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346514       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346517       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.346523       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346527       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:21:42.346530       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:21:42.346534       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:17.599674       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:22:17.599770       1 status.go:500] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.599905       1 status.go:502] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.599938       1 status.go:542] Node ip-10-0-74-240.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.599967       1 status.go:500] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.599996       1 status.go:502] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.600023       1 status.go:542] Node ip-10-0-12-37.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.600052       1 status.go:500] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.600081       1 status.go:502] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.600108       1 status.go:542] Node ip-10-0-49-30.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.600137       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:17.600166       1 node_controller.go:1248] maxUnavailable is 1 and unavail is 0 (getAllCandidateMachines)
I0120 21:22:17.600193       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:22:17.600226       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:22:17.601976       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:22:17.602033       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602060       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602074       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.602081       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602085       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602089       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.602093       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602097       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.602100       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.602104       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:17.602110       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:22:17.602114       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:22:17.602122       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:22:17.623383       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:22:17.623405       1 status.go:500] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623412       1 status.go:502] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623417       1 status.go:542] Node ip-10-0-12-37.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.623421       1 status.go:500] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623425       1 status.go:502] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623429       1 status.go:542] Node ip-10-0-49-30.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.623433       1 status.go:500] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623436       1 status.go:502] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:22:17.623440       1 status.go:542] Node ip-10-0-74-240.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.623443       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:17.624937       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:22:17.624999       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625029       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625059       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.625088       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625117       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625151       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.625178       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625206       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:17.625234       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:22:17.625262       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:35.392923       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool master
I0120 21:22:35.547123       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool worker
I0120 21:22:35.784320       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool master
I0120 21:22:35.937990       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool worker
I0120 21:22:44.684957       1 node_controller.go:1012] Requeueing layered pool worker: Desired Image not set in MachineOSBuild
I0120 21:22:44.703450       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:22:44.703472       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703479       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703484       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:22:44.703488       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703493       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703496       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:22:44.703501       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703505       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:44.703509       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:22:44.703513       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:22:49.732214       1 node_controller.go:1012] Requeueing layered pool worker: Desired Image not set in MachineOSBuild
I0120 21:22:49.752028       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:22:49.752051       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752056       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752061       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:22:49.752066       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752070       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752074       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:22:49.752079       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752083       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:22:49.752086       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:22:49.752090       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:26:59.023131       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:26:59.023155       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023163       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023167       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023172       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023176       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023180       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023185       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023188       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023192       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023195       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0

I0120 21:26:59.023200       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:26:59.023204       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:26:59.023210       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-46-217.ec2.internal
I0120 21:26:59.023215       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-5-10.ec2.internal
I0120 21:26:59.023222       1 node_controller.go:1261] Already picked 2 nodes, capacity is 2, stopping

I0120 21:26:59.023228       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:26:59.023239       1 node_controller.go:1076] worker: 2 candidate nodes in 2 zones for update, capacity: 2
I0120 21:26:59.023271       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:26:59.023280       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023285       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023288       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023292       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023296       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023300       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023304       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023310       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.023314       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.023318       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:26:59.023323       1 node_controller.go:1378] Selected node ip-10-0-46-217.ec2.internal for update (current selection count: 1) [updateCandidateMachines]
I0120 21:26:59.023327       1 node_controller.go:1378] Selected node ip-10-0-5-10.ec2.internal for update (current selection count: 2) [updateCandidateMachines]
I0120 21:26:59.023332       1 node_controller.go:1382] Final list of nodes to update in pool worker: 2 nodes (capacity: 2) [updateCandidateMachines]
I0120 21:26:59.030142       1 node_controller.go:1399] Continuing to sync layered MachineConfigPool worker
I0120 21:26:59.034188       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-46-217.ec2.internal, pool=worker, layered=true, mosbIsNil=false
I0120 21:26:59.050769       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/desiredImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:26:59.057684       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-5-10.ec2.internal, pool=worker, layered=true, mosbIsNil=false
I0120 21:26:59.076535       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"205758", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfigAndOSImage' Set target for 2 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6 / Image: image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:26:59.076708       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/desiredImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:26:59.084666       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:26:59.212149       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:26:59.212173       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212181       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212185       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.212190       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212194       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212197       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.212201       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212205       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:26:59.212208       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:26:59.212212       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:27:00.992450       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:27:01.096067       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:27:04.086378       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:27:04.110510       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:27:04.110619       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:04.110647       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:04.110676       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:27:04.110696       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:27:04.110702       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:04.110707       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:27:04.110712       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:04.110716       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:27:04.112305       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:27:04.161850       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:27:04.161873       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:04.161878       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:04.161883       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:27:04.161887       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:27:04.161891       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:04.161896       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:27:04.161900       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:04.161903       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:27:06.102044       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: cordoning
I0120 21:27:06.102109       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:27:06.125641       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:27:06.141078       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:27:06.152715       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating drain
I0120 21:27:06.204990       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: cordoning
I0120 21:27:06.205011       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:27:06.224858       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:27:06.242882       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:27:06.248112       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating drain
E0120 21:27:08.010177       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-vl8gn, openshift-cluster-node-tuning-operator/tuned-qslsp, openshift-dns/dns-default-vvzxt, openshift-dns/node-resolver-5srtq, openshift-image-registry/node-ca-9psxx, openshift-ingress-canary/ingress-canary-l6pth, openshift-insights/insights-runtime-extractor-hl5q4, openshift-machine-config-operator/machine-config-daemon-rmtb2, openshift-monitoring/node-exporter-vb9lg, openshift-multus/multus-additional-cni-plugins-xc5b9, openshift-multus/multus-mp8bq, openshift-multus/network-metrics-daemon-c8vng, openshift-network-diagnostics/network-check-target-vmvjd, openshift-network-operator/iptables-alerter-zhxcz, openshift-ovn-kubernetes/ovnkube-node-qc66q
I0120 21:27:08.011956       1 drain_controller.go:153] evicting pod openshift-network-diagnostics/network-check-source-77c8b75fd4-xhx4j
I0120 21:27:08.011960       1 drain_controller.go:153] evicting pod openshift-kube-storage-version-migrator/migrator-6dcd57bc4f-l6qqp
I0120 21:27:08.011971       1 drain_controller.go:153] evicting pod openshift-cluster-api/capi-controller-manager-8598896b96-7k4rp
I0120 21:27:08.011972       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
I0120 21:27:08.011978       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-kk5sk
I0120 21:27:08.011981       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-1
I0120 21:27:08.011981       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-49p2s
I0120 21:27:08.011987       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-lnqjp
I0120 21:27:08.011988       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-g247h
I0120 21:27:08.011993       1 drain_controller.go:153] evicting pod openshift-insights/periodic-gathering-ksjbs-rpzds
I0120 21:27:08.011992       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-cn2wt
I0120 21:27:08.011996       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-sdwmj
I0120 21:27:08.011999       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-9mwr9
I0120 21:27:08.095718       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-insights/periodic-gathering-ksjbs-rpzds
I0120 21:27:09.113869       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:27:09.113892       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:27:09.113897       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:09.113905       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:27:09.113909       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:09.113914       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:09.113919       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:09.113923       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:27:09.113927       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:27:09.140105       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:27:09.140128       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:09.140136       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:27:09.140140       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:27:09.140145       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:27:09.140149       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:09.140154       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:27:09.140157       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:27:09.140161       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
E0120 21:27:10.218306       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-w2bqn, openshift-cluster-node-tuning-operator/tuned-r9wt5, openshift-dns/dns-default-kh6dw, openshift-dns/node-resolver-sxwkk, openshift-image-registry/node-ca-cq7nn, openshift-ingress-canary/ingress-canary-mhb88, openshift-insights/insights-runtime-extractor-sd5zx, openshift-machine-config-operator/machine-config-daemon-mxfsq, openshift-monitoring/node-exporter-tn6kd, openshift-multus/multus-additional-cni-plugins-hrr9q, openshift-multus/multus-pc2gv, openshift-multus/network-metrics-daemon-v7nr4, openshift-network-diagnostics/network-check-target-drltj, openshift-network-operator/iptables-alerter-f2gl5, openshift-ovn-kubernetes/ovnkube-node-f9gcr
I0120 21:27:10.220078       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-dzm4g
I0120 21:27:10.220121       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-lr9q6
I0120 21:27:10.220134       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:27:10.220244       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-gpml7
I0120 21:27:10.220289       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:27:10.220360       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-7ql87
I0120 21:27:10.220255       1 drain_controller.go:153] evicting pod openshift-monitoring/kube-state-metrics-7789cb954-kbmqb
I0120 21:27:10.220263       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
I0120 21:27:10.220271       1 drain_controller.go:153] evicting pod openshift-monitoring/telemeter-client-55469dfcdf-9gts8
I0120 21:27:10.220277       1 drain_controller.go:153] evicting pod openshift-monitoring/openshift-state-metrics-798f7c49-dp6lr
I0120 21:27:10.220283       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-6ccdb
I0120 21:27:10.220647       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-fwdvs
E0120 21:27:10.237492       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-7h9lh" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:10.243572       1 drain_controller.go:153] error when evicting pods/"monitoring-plugin-57b46cfd5c-lr9q6" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:10.256045       1 request.go:700] Waited for 1.126318218s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/pods/router-default-787b98474-lnqjp
E0120 21:27:10.335584       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:10.335584       1 drain_controller.go:153] error when evicting pods/"thanos-querier-68d8f8f746-7ql87" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:10.335593       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:10.335673       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-gpml7" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:10.838610       1 drain_controller.go:153] error when evicting pods/"router-default-787b98474-6ccdb" -n "openshift-ingress" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:10.860488       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-sdwmj
I0120 21:27:11.059903       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-9mwr9
E0120 21:27:11.090547       1 drain_controller.go:153] error when evicting pods/"prometheus-operator-admission-webhook-5d9668865-fwdvs" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:11.257344       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-network-diagnostics/network-check-source-77c8b75fd4-xhx4j
I0120 21:27:11.456909       1 request.go:700] Waited for 1.371044171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-api/pods/capi-controller-manager-8598896b96-7k4rp
I0120 21:27:11.466459       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-cluster-api/capi-controller-manager-8598896b96-7k4rp
I0120 21:27:11.658553       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-1
I0120 21:27:11.859788       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-49p2s
I0120 21:27:12.059391       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-0
I0120 21:27:12.653305       1 request.go:700] Waited for 2.396843186s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-console/pods/networking-console-plugin-5f88d5854c-dzm4g
I0120 21:27:12.660289       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-dzm4g
I0120 21:27:12.880923       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/kube-state-metrics-7789cb954-kbmqb
I0120 21:27:13.057831       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/telemeter-client-55469dfcdf-9gts8
I0120 21:27:13.255860       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/openshift-state-metrics-798f7c49-dp6lr
I0120 21:27:13.852931       1 request.go:700] Waited for 2.511510129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-operator-admission-webhook-5d9668865-cn2wt
I0120 21:27:13.856073       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-cn2wt
I0120 21:27:15.238458       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
I0120 21:27:15.244599       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-lr9q6
E0120 21:27:15.247613       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-7h9lh" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:15.336686       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-gpml7
I0120 21:27:15.336958       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:27:15.337067       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-7ql87
I0120 21:27:15.337158       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:27:15.348229       1 drain_controller.go:153] error when evicting pods/"thanos-querier-68d8f8f746-7ql87" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:15.349672       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:15.358135       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:15.358146       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-gpml7" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:15.839458       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-6ccdb
I0120 21:27:16.091290       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-fwdvs
I0120 21:27:17.257965       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-lr9q6
I0120 21:27:18.254302       1 request.go:700] Waited for 1.123919358s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-g247h
I0120 21:27:18.458088       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-fwdvs
I0120 21:27:20.248182       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
E0120 21:27:20.255947       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-7h9lh" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:20.349721       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-7ql87
I0120 21:27:20.352828       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:27:20.359022       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:27:20.359177       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-gpml7
E0120 21:27:20.369273       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:20.370678       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-gpml7" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:20.380282       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:22.253729       1 request.go:700] Waited for 1.123709064s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-g247h
I0120 21:27:23.256726       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-7ql87
I0120 21:27:24.253355       1 request.go:700] Waited for 1.122780946s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-g247h
I0120 21:27:25.256425       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
E0120 21:27:25.264893       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-7h9lh" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:25.369333       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:27:25.371474       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-gpml7
E0120 21:27:25.377802       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:27:25.378187       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-gpml7" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:25.381294       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:27:25.478269       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:30.265769       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
I0120 21:27:30.378201       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:27:30.378516       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-gpml7
E0120 21:27:30.387448       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:30.478852       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:27:30.491940       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:31.454329       1 request.go:700] Waited for 1.037730388s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-image-registry/pods/image-registry-c66456f44-gpml7
I0120 21:27:34.253691       1 request.go:700] Waited for 1.123605419s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/pods/router-default-787b98474-lnqjp
I0120 21:27:35.387920       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:27:35.396782       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:35.453851       1 request.go:700] Waited for 1.351482975s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-image-registry/pods/image-registry-c66456f44-kk5sk
I0120 21:27:35.456575       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-kk5sk
I0120 21:27:35.492735       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:27:35.502629       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:37.253355       1 request.go:700] Waited for 1.122400487s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-g247h
I0120 21:27:39.856373       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-kube-storage-version-migrator/migrator-6dcd57bc4f-l6qqp
I0120 21:27:40.253470       1 request.go:700] Waited for 1.120697615s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-g247h
I0120 21:27:40.397584       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:27:40.405648       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:40.503128       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:27:40.514510       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:45.406660       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:27:45.414836       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:45.515120       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:27:48.257702       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-0
I0120 21:27:50.414947       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:27:50.424920       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:55.425265       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:27:55.434699       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:27:58.057290       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-gpml7
I0120 21:28:00.134525       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-lnqjp
I0120 21:28:00.435391       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:28:00.443676       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:28:05.444637       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:28:05.453339       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:28:10.454050       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:28:10.462025       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:28:15.462889       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:28:22.510399       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-1
I0120 21:28:33.064858       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-6ccdb
I0120 21:28:38.135339       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-g247h" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s
I0120 21:28:40.264347       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-7h9lh" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s
I0120 21:29:38.135867       1 drain_controller.go:380] Previous node drain found. Drain has been going on for 0.042225049974166665 hours
I0120 21:29:38.135894       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating drain
E0120 21:29:39.178046       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-vl8gn, openshift-cluster-node-tuning-operator/tuned-qslsp, openshift-dns/dns-default-vvzxt, openshift-dns/node-resolver-5srtq, openshift-image-registry/node-ca-9psxx, openshift-ingress-canary/ingress-canary-l6pth, openshift-insights/insights-runtime-extractor-hl5q4, openshift-machine-config-operator/machine-config-daemon-rmtb2, openshift-monitoring/node-exporter-vb9lg, openshift-multus/multus-additional-cni-plugins-xc5b9, openshift-multus/multus-mp8bq, openshift-multus/network-metrics-daemon-c8vng, openshift-network-diagnostics/network-check-target-vmvjd, openshift-network-operator/iptables-alerter-zhxcz, openshift-ovn-kubernetes/ovnkube-node-qc66q
I0120 21:29:39.179712       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-g247h
I0120 21:29:40.197619       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/metrics-server-69d8d5dd44-g247h
I0120 21:29:40.217558       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: operation successful; applying completion annotation
I0120 21:29:40.264655       1 drain_controller.go:380] Previous node drain found. Drain has been going on for 0.04278880547333333 hours
I0120 21:29:40.264741       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating drain
E0120 21:29:42.178869       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-w2bqn, openshift-cluster-node-tuning-operator/tuned-r9wt5, openshift-dns/dns-default-kh6dw, openshift-dns/node-resolver-sxwkk, openshift-image-registry/node-ca-cq7nn, openshift-ingress-canary/ingress-canary-mhb88, openshift-insights/insights-runtime-extractor-sd5zx, openshift-machine-config-operator/machine-config-daemon-mxfsq, openshift-monitoring/node-exporter-tn6kd, openshift-multus/multus-additional-cni-plugins-hrr9q, openshift-multus/multus-pc2gv, openshift-multus/network-metrics-daemon-v7nr4, openshift-network-diagnostics/network-check-target-drltj, openshift-network-operator/iptables-alerter-f2gl5, openshift-ovn-kubernetes/ovnkube-node-f9gcr
I0120 21:29:42.180569       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
I0120 21:30:02.200057       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/metrics-server-69d8d5dd44-7h9lh
I0120 21:30:02.220475       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: operation successful; applying completion annotation
I0120 21:30:59.942092       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting OutOfDisk=Unknown
I0120 21:30:59.986567       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:04.957883       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:04.957908       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:04.957914       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:04.957920       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:04.957925       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:04.957929       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:04.957933       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:04.957936       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:04.957941       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:04.990928       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:04.990950       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:04.990955       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:04.990962       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:04.990965       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:04.990970       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:04.990974       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:04.990979       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:04.990983       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:05.633878       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:10.650225       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:10.650246       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:10.650251       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:10.650258       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:10.650270       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:10.650277       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:10.650282       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:10.650286       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:10.650291       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:10.670464       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:10.670486       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:10.670491       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:10.670499       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:10.670503       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:10.670508       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:10.670512       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:10.670516       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:10.670520       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:13.651444       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting NotReady=False
I0120 21:31:13.685774       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:13.717280       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:15.555300       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:15.590248       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:18.667758       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:18.667781       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:18.667786       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:18.667794       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:18.667799       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:18.667803       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:18.667807       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:18.667810       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:18.667814       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:18.691457       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:18.691478       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:18.691483       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:18.691491       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:18.691496       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:18.691500       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:18.691504       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:18.691507       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:18.691511       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:25.610394       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting OutOfDisk=Unknown
I0120 21:31:25.653391       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:27.137131       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting Unschedulable
I0120 21:31:27.160666       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:30.630885       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:30.630908       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:30.630914       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:30.630919       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:30.630924       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:30.630928       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:30.630934       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:30.630938       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:30.630942       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:30.732184       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:30.732208       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:30.732215       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:30.732220       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:30.732226       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:30.732230       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:30.732235       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:30.732239       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:30.732280       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:31.251836       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:31.307367       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:36.269554       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:36.269578       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:36.269584       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:36.269589       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:36.269592       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:36.269597       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:36.269602       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:36.269606       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:36.269611       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:36.293576       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:36.293599       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:36.293604       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:36.293609       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:31:36.293614       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:36.293619       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:36.293623       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:36.293627       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:36.293631       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:36.865897       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting NotReady=False
I0120 21:31:36.897921       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:36.923127       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:37.051546       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: uncordoning
I0120 21:31:37.051591       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating uncordon (currently schedulable: false)
I0120 21:31:37.072659       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: uncordon succeeded (currently schedulable: true)
I0120 21:31:37.092921       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: operation successful; applying completion annotation
I0120 21:31:37.108129       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:31:41.273797       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:41.308847       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:41.888180       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:41.888202       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:31:41.888207       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:41.888213       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:41.888218       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:41.888222       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:41.888228       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:41.888231       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:41.888237       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:41.909976       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:41.910001       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:41.910005       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:41.910011       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:31:41.910015       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:41.910020       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:41.910024       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:41.910029       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:41.910032       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:31:42.076006       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Done
I0120 21:31:42.076564       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/currentImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:31:47.091181       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:47.091204       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091211       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091215       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.091220       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091224       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091228       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.091234       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:47.091238       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:47.091243       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:47.091248       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:31:47.091253       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:31:47.091260       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:31:47.091266       1 node_controller.go:1261] Already picked 1 nodes, capacity is 1, stopping
I0120 21:31:47.091272       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:31:47.091282       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:31:47.091314       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:31:47.091322       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091327       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091331       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.091335       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:47.091338       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:31:47.091342       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091346       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.091350       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.091354       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:47.091359       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:31:47.091363       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:31:47.101006       1 node_controller.go:1399] Continuing to sync layered MachineConfigPool worker
I0120 21:31:47.101134       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"207984", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfigAndOSImage' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6 / Image: image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:31:47.107366       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:31:47.284060       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:47.284086       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:47.284091       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:47.284099       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.284103       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.284107       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.284111       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.284115       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:47.284119       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:47.284123       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:51.356410       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting Unschedulable
I0120 21:31:51.393296       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:31:52.313020       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:52.313068       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313074       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313079       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.313084       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313094       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313097       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.313102       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:52.313106       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:52.313111       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:52.313116       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:31:52.313120       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:31:52.313127       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:31:52.313135       1 node_controller.go:1261] Already picked 1 nodes, capacity is 1, stopping
I0120 21:31:52.313141       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:31:52.313152       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:31:52.313202       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:31:52.313212       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313217       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313221       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.313225       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:52.313306       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:31:52.313349       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313368       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.313375       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.313379       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:52.313385       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:31:52.313390       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:31:52.323569       1 node_controller.go:1399] Continuing to sync layered MachineConfigPool worker
I0120 21:31:52.323684       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"212304", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfigAndOSImage' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6 / Image: image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:31:52.331571       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:31:52.502921       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:31:52.502946       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:31:52.502951       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:31:52.502957       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.502961       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.502965       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.502971       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.502975       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:31:52.502977       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:31:52.502981       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:31:56.327592       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:32:00.411278       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: uncordoning
I0120 21:32:00.411323       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating uncordon (currently schedulable: false)
I0120 21:32:00.434169       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: uncordon succeeded (currently schedulable: true)
I0120 21:32:00.468406       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:32:00.468516       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: operation successful; applying completion annotation
I0120 21:32:01.352408       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:01.352431       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:32:01.352437       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:01.352443       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352448       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352453       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.352457       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352461       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352464       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.352469       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:01.352474       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:32:01.352478       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:32:01.352485       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:32:01.352492       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:32:01.352502       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:32:01.352535       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:32:01.352542       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:32:01.352547       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:32:01.352551       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352555       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352559       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.352565       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352569       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.352572       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.352576       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:01.352581       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:32:01.352586       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:32:01.359168       1 node_controller.go:1399] Continuing to sync layered MachineConfigPool worker
I0120 21:32:01.359300       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"212304", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfigAndOSImage' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6 / Image: image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:32:01.365177       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:32:01.533807       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:01.533829       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.533835       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.533840       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.533845       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.533849       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:01.533852       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:01.533856       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:32:01.533860       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:01.533864       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:05.433432       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Done
I0120 21:32:05.433540       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/currentImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:32:10.448354       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:10.448376       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448382       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448387       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448392       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448396       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448399       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448404       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448407       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448411       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448414       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:32:10.448419       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:32:10.448423       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:32:10.448430       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:32:10.448437       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:32:10.448446       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 2
I0120 21:32:10.448480       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:32:10.448488       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448493       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448498       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448501       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448505       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448509       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448512       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448516       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.448519       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.448523       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:32:10.448528       1 node_controller.go:1378] Selected node ip-10-0-86-209.ec2.internal for update (current selection count: 1) [updateCandidateMachines]
I0120 21:32:10.448533       1 node_controller.go:1382] Final list of nodes to update in pool worker: 1 nodes (capacity: 2) [updateCandidateMachines]
I0120 21:32:10.455576       1 node_controller.go:1399] Continuing to sync layered MachineConfigPool worker
I0120 21:32:10.460667       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-86-209.ec2.internal, pool=worker, layered=true, mosbIsNil=false
I0120 21:32:10.474995       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"212304", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfigAndOSImage' Targeted node ip-10-0-86-209.ec2.internal to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6 / Image: image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:32:10.476783       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed annotation machineconfiguration.openshift.io/desiredImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:32:10.494284       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:32:10.641063       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:10.641086       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641093       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641098       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.641102       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641107       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641110       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.641114       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641118       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:10.641121       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:32:10.641125       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:32:12.088994       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:32:15.509278       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:15.509302       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.509307       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.509312       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:15.509317       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.509321       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.509325       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:15.509330       1 status.go:495] Node ip-10-0-86-209.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:32:15.509334       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:15.509341       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:15.509347       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:32:15.509350       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:32:15.509361       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:32:15.509581       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:32:15.586937       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:15.586961       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.586967       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.586972       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:15.586976       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.586980       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:15.586984       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:15.586989       1 status.go:495] Node ip-10-0-86-209.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:32:15.586992       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:15.586996       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:17.195908       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: cordoning
I0120 21:32:17.195932       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:32:17.223387       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:32:17.249101       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:32:17.254141       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating drain
E0120 21:32:18.327587       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-8khhk, openshift-cluster-node-tuning-operator/tuned-g7zp9, openshift-dns/dns-default-mk56w, openshift-dns/node-resolver-hwqv4, openshift-image-registry/node-ca-8msd2, openshift-ingress-canary/ingress-canary-5nfvj, openshift-insights/insights-runtime-extractor-tc9l7, openshift-machine-config-operator/machine-config-daemon-m6t4q, openshift-monitoring/node-exporter-f4mcq, openshift-multus/multus-6kxrr, openshift-multus/multus-additional-cni-plugins-97f84, openshift-multus/network-metrics-daemon-cjxq7, openshift-network-diagnostics/network-check-target-h5k4g, openshift-network-operator/iptables-alerter-x2bn5, openshift-ovn-kubernetes/ovnkube-node-9n6w2
I0120 21:32:18.329375       1 drain_controller.go:153] evicting pod openshift-cluster-api/capi-controller-manager-8598896b96-sg2cj
I0120 21:32:18.329397       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-ld64r
I0120 21:32:18.329413       1 drain_controller.go:153] evicting pod openshift-kube-storage-version-migrator/migrator-6dcd57bc4f-nq78n
I0120 21:32:18.329440       1 drain_controller.go:153] evicting pod openshift-operator-lifecycle-manager/collect-profiles-28956810-pmpb5
I0120 21:32:18.329520       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-7l4qm
I0120 21:32:18.329533       1 drain_controller.go:153] evicting pod openshift-console/downloads-6ffc5555fc-nlq7r
I0120 21:32:18.329587       1 drain_controller.go:153] evicting pod openshift-monitoring/openshift-state-metrics-798f7c49-qr6zv
I0120 21:32:18.329617       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-kbfqs
I0120 21:32:18.329627       1 drain_controller.go:153] evicting pod openshift-monitoring/telemeter-client-55469dfcdf-rv594
I0120 21:32:18.329699       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-458wh
I0120 21:32:18.329708       1 drain_controller.go:153] evicting pod openshift-operator-lifecycle-manager/collect-profiles-28956780-4j452
I0120 21:32:18.329732       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-wfl4n
I0120 21:32:18.329619       1 drain_controller.go:153] evicting pod openshift-machine-config-operator/machine-os-builder-b76f56d79-skclc
I0120 21:32:18.329774       1 drain_controller.go:153] evicting pod openshift-monitoring/kube-state-metrics-7789cb954-pf8sf
I0120 21:32:18.329602       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-rcp6v
I0120 21:32:18.329820       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-5d8qj
I0120 21:32:18.329525       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-zdpvv
I0120 21:32:18.329758       1 drain_controller.go:153] evicting pod openshift-network-diagnostics/network-check-source-77c8b75fd4-kmg2b
I0120 21:32:18.329609       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-1
I0120 21:32:18.329611       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
I0120 21:32:18.329766       1 drain_controller.go:153] evicting pod openshift-operator-lifecycle-manager/collect-profiles-28956795-5j6l4
I0120 21:32:18.402153       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-operator-lifecycle-manager/collect-profiles-28956810-pmpb5
I0120 21:32:18.447625       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-console/downloads-6ffc5555fc-nlq7r
I0120 21:32:18.631951       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-operator-lifecycle-manager/collect-profiles-28956780-4j452
I0120 21:32:19.332570       1 request.go:700] Waited for 1.002686252s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-5d8qj/eviction
I0120 21:32:20.530241       1 request.go:700] Waited for 2.200128169s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-operator-lifecycle-manager/pods/collect-profiles-28956795-5j6l4/eviction
I0120 21:32:20.930887       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:20.930911       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:20.930917       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:20.930921       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:20.930925       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:20.930930       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:20.930933       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:20.930938       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:32:20.930941       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:20.930945       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:20.930951       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:32:20.930955       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:32:20.930964       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:32:21.036972       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:32:21.036994       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:21.037000       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:21.037005       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:32:21.037010       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:21.037016       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:32:21.037019       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:32:21.037023       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:32:21.037027       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:32:21.037031       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:32:21.579494       1 request.go:700] Waited for 1.690831725s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-machine-config-operator/pods/machine-os-builder-b76f56d79-skclc
I0120 21:32:22.775564       1 request.go:700] Waited for 2.356913687s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-storage-version-migrator/pods/migrator-6dcd57bc4f-nq78n
I0120 21:32:23.603084       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-1
I0120 21:32:23.791681       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-operator-lifecycle-manager/collect-profiles-28956795-5j6l4
I0120 21:32:23.977349       1 request.go:700] Waited for 2.972019725s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-5d8qj
I0120 21:32:24.179025       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-kbfqs
I0120 21:32:24.378903       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/openshift-state-metrics-798f7c49-qr6zv
I0120 21:32:24.578752       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-ld64r
I0120 21:32:24.778241       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-machine-config-operator/machine-os-builder-b76f56d79-skclc
I0120 21:32:24.977870       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/kube-state-metrics-7789cb954-pf8sf
I0120 21:32:25.174964       1 request.go:700] Waited for 2.88445679s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-console/pods/networking-console-plugin-5f88d5854c-rcp6v
I0120 21:32:25.178392       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-rcp6v
I0120 21:32:25.579094       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-diagnostics/network-check-source-77c8b75fd4-kmg2b
I0120 21:32:25.978178       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-cluster-api/capi-controller-manager-8598896b96-sg2cj
I0120 21:32:26.175483       1 request.go:700] Waited for 2.763361506s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/telemeter-client-55469dfcdf-rv594
I0120 21:32:26.178912       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/telemeter-client-55469dfcdf-rv594
I0120 21:32:27.575140       1 request.go:700] Waited for 1.156942259s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-storage-version-migrator/pods/migrator-6dcd57bc4f-nq78n
I0120 21:32:31.353928       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-wfl4n
I0120 21:32:31.362345       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-458wh
I0120 21:32:31.376155       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-0
I0120 21:32:46.459114       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-7l4qm
I0120 21:32:50.421499       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-kube-storage-version-migrator/migrator-6dcd57bc4f-nq78n
I0120 21:33:37.385327       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-zdpvv
I0120 21:33:49.007305       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-5d8qj" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s
I0120 21:34:49.007474       1 drain_controller.go:380] Previous node drain found. Drain has been going on for 0.042162233666944444 hours
I0120 21:34:49.007500       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating drain
E0120 21:34:50.052524       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-8khhk, openshift-cluster-node-tuning-operator/tuned-g7zp9, openshift-dns/dns-default-mk56w, openshift-dns/node-resolver-hwqv4, openshift-image-registry/node-ca-8msd2, openshift-ingress-canary/ingress-canary-5nfvj, openshift-insights/insights-runtime-extractor-tc9l7, openshift-machine-config-operator/machine-config-daemon-m6t4q, openshift-monitoring/node-exporter-f4mcq, openshift-multus/multus-6kxrr, openshift-multus/multus-additional-cni-plugins-97f84, openshift-multus/network-metrics-daemon-cjxq7, openshift-network-diagnostics/network-check-target-h5k4g, openshift-network-operator/iptables-alerter-x2bn5, openshift-ovn-kubernetes/ovnkube-node-9n6w2
I0120 21:34:50.054263       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-5d8qj
I0120 21:34:51.071004       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/metrics-server-69d8d5dd44-5d8qj
I0120 21:34:51.088971       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: operation successful; applying completion annotation
I0120 21:36:06.392018       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: Reporting unready: node ip-10-0-86-209.ec2.internal is reporting OutOfDisk=Unknown
I0120 21:36:06.434214       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:11.410080       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:11.410102       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.410109       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.410113       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:11.410120       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:11.410124       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:11.410129       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.410133       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.410137       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:11.410140       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:11.410146       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:36:11.410149       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:36:11.410159       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:36:11.432004       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:11.432026       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.432033       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.432038       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:11.432042       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.432046       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:11.432049       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:11.432055       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:11.432059       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:11.432063       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:12.252719       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:17.266775       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:17.266798       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.266805       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.266809       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:17.266815       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.266819       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.266822       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:17.266828       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:17.266832       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:17.266836       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:17.266841       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:36:17.266845       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:36:17.266854       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:36:17.287801       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:17.287824       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.287830       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.287835       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:17.287841       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:17.287845       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:17.287850       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.287854       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:17.287858       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:17.287862       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:23.978961       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: Reporting unready: node ip-10-0-86-209.ec2.internal is reporting NotReady=False
I0120 21:36:24.010686       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:24.032889       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:27.158536       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:27.182037       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:29.009454       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:29.009497       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.009504       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.009509       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:29.009513       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.009517       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.009521       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:29.009527       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:29.009531       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:29.009535       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:29.009540       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:36:29.009544       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:36:29.009553       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:36:29.031347       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:29.031371       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.031377       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.031381       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:29.031385       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.031390       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:29.031394       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:29.031400       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:29.031404       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:29.031408       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:38.372286       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: Reporting unready: node ip-10-0-86-209.ec2.internal is reporting Unschedulable
I0120 21:36:38.420884       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:42.205158       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:43.389316       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:43.389341       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.389348       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.389352       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:43.389357       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.389361       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.389365       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:43.389370       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:43.389373       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:43.389378       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:43.389382       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:36:43.389386       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:36:43.389395       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:36:43.482578       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:43.482600       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:36:43.482605       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=true
I0120 21:36:43.482610       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.482614       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.482618       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:43.482622       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.482626       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:43.482630       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:43.482634       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:36:47.399112       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: uncordoning
I0120 21:36:47.399155       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating uncordon (currently schedulable: false)
I0120 21:36:47.424863       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: uncordon succeeded (currently schedulable: true)
I0120 21:36:47.443926       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: operation successful; applying completion annotation
I0120 21:36:47.455581       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:36:52.423574       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Done
I0120 21:36:52.423606       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed annotation machineconfiguration.openshift.io/currentImage = image-registry.openshift-image-registry.svc:5000/openshift-machine-config-operator/ocb-image@sha256:eb1974454fda480b3bbcfdf1747bdb23bea394055e2fdbc3929b2a794fbbb150
I0120 21:36:52.478483       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:52.478587       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478599       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478605       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.478610       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478614       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478617       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.478622       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478626       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.478630       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.478633       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:36:52.478638       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:36:52.478642       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:36:52.478651       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:36:52.567100       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:52.567123       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567129       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567134       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.567138       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567142       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567146       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.567149       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567154       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:52.567157       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:52.567161       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:36:57.593906       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:57.593929       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593937       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593942       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.593946       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593951       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593956       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.593960       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593964       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.593967       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.593972       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:36:57.593977       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:36:57.593981       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:36:57.593990       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:36:57.612434       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=true)
I0120 21:36:57.612455       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612460       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612465       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.612469       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612473       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612476       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.612480       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612484       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:36:57.612488       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:36:57.612492       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:37:31.402313       1 template_controller.go:146] Re-syncing ControllerConfig due to secret pull-secret change
I0120 21:38:12.681554       1 start.go:61] Version: 649ff75f-dirty (ad0657ecb77b5d5aca6b18b95146ced293f83b75)
I0120 21:38:12.681781       1 leaderelection.go:121] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
I0120 21:38:12.705712       1 leaderelection.go:254] attempting to acquire leader lease openshift-machine-config-operator/machine-config-controller...
I0120 21:38:12.719776       1 leaderelection.go:268] successfully acquired lease openshift-machine-config-operator/machine-config-controller
I0120 21:38:12.720544       1 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0120 21:38:12.720744       1 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0120 21:38:12.737946       1 metrics.go:92] Registering Prometheus metrics
I0120 21:38:12.737997       1 metrics.go:99] Starting metrics listener on 127.0.0.1:8797
I0120 21:38:12.738029       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
I0120 21:38:12.750464       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go/informers/factory.go:160
I0120 21:38:12.750861       1 reflector.go:368] Caches populated for *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.753239       1 reflector.go:368] Caches populated for *v1.ControllerConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.757320       1 reflector.go:368] Caches populated for *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.757449       1 event.go:377] Event(v1.ObjectReference{Kind:"Node", Namespace:"openshift-machine-config-operator", Name:"ip-10-0-12-37.ec2.internal", UID:"3966fb07-ed60-4892-92f4-2597e9c54157", APIVersion:"v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AWSClusterHostedDNS", "AWSEFSDriverVolumeMetrics", "AdditionalRoutingCapabilities", "AdminNetworkPolicy", "AlibabaPlatform", "AutomatedEtcdBackup", "AzureWorkloadIdentity", "BareMetalLoadBalancer", "BootcNodeManagement", "BuildCSIVolumes", "CSIDriverSharedResource", "ChunkSizeMiB", "CloudDualStackNodeIPs", "ClusterMonitoringConfig", "ConsolePluginContentSecurityPolicy", "DNSNameResolver", "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EtcdBackendQuota", "Example", "ExternalOIDC", "GCPClusterHostedDNS", "GCPLabelsTags", "HardwareSpeed", "ImageStreamImportMode", "IngressControllerDynamicConfigurationManager", "IngressControllerLBSubnetsAWS", "InsightsConfig", "InsightsConfigAPI", "InsightsOnDemandDataGather", "InsightsRuntimeExtractor", "KMSv1", "MachineAPIProviderOpenStack", "MachineConfigNodes", "ManagedBootImages", "ManagedBootImagesAWS", "MaxUnavailableStatefulSet", "MetricsCollectionProfiles", "MinimumKubeletVersion", "MixedCPUsAllocation", "MultiArchInstallAWS", "MultiArchInstallGCP", "NetworkDiagnosticsConfig", "NetworkLiveMigration", "NetworkSegmentation", "NewOLM", "NodeDisruptionPolicy", "NodeSwap", "NutanixMultiSubnets", "OVNObservability", "OnClusterBuild", "OpenShiftPodSecurityAdmission", "PersistentIPsForVirtualization", "PinnedImages", "PlatformOperators", "PrivateHostedZoneAWS", "ProcMountType", "RouteAdvertisements", "RouteExternalCertificate", "ServiceAccountTokenNodeBinding", "SetEIPForNLBIngressController", "SignatureStores", "SigstoreImageVerification", "TranslateStreamCloseWebsocketRequests", "UpgradeStatus", "UserNamespacesPodSecurityStandards", "UserNamespacesSupport", "VSphereControlPlaneMachineSet", "VSphereDriverConfiguration", "VSphereMultiNetworks", "VSphereMultiVCenters", "VSphereStaticIPs", "ValidatingAdmissionPolicy", "VolumeAttributesClass", "VolumeGroupSnapshot"}, Disabled:[]v1.FeatureGateName{"ClusterAPIInstall", "ClusterAPIInstallIBMCloud", "EventedPLEG", "GatewayAPI", "MachineAPIMigration", "MachineAPIOperatorDisableMachineHealthCheckController", "MultiArchInstallAzure"}}
I0120 21:38:12.757485       1 start.go:105] FeatureGates initialized: enabled=[AWSClusterHostedDNS AWSEFSDriverVolumeMetrics AdditionalRoutingCapabilities AdminNetworkPolicy AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BareMetalLoadBalancer BootcNodeManagement BuildCSIVolumes CSIDriverSharedResource ChunkSizeMiB CloudDualStackNodeIPs ClusterMonitoringConfig ConsolePluginContentSecurityPolicy DNSNameResolver DisableKubeletCloudCredentialProviders DynamicResourceAllocation EtcdBackendQuota Example ExternalOIDC GCPClusterHostedDNS GCPLabelsTags HardwareSpeed ImageStreamImportMode IngressControllerDynamicConfigurationManager IngressControllerLBSubnetsAWS InsightsConfig InsightsConfigAPI InsightsOnDemandDataGather InsightsRuntimeExtractor KMSv1 MachineAPIProviderOpenStack MachineConfigNodes ManagedBootImages ManagedBootImagesAWS MaxUnavailableStatefulSet MetricsCollectionProfiles MinimumKubeletVersion MixedCPUsAllocation MultiArchInstallAWS MultiArchInstallGCP NetworkDiagnosticsConfig NetworkLiveMigration NetworkSegmentation NewOLM NodeDisruptionPolicy NodeSwap NutanixMultiSubnets OVNObservability OnClusterBuild OpenShiftPodSecurityAdmission PersistentIPsForVirtualization PinnedImages PlatformOperators PrivateHostedZoneAWS ProcMountType RouteAdvertisements RouteExternalCertificate ServiceAccountTokenNodeBinding SetEIPForNLBIngressController SignatureStores SigstoreImageVerification TranslateStreamCloseWebsocketRequests UpgradeStatus UserNamespacesPodSecurityStandards UserNamespacesSupport VSphereControlPlaneMachineSet VSphereDriverConfiguration VSphereMultiNetworks VSphereMultiVCenters VSphereStaticIPs ValidatingAdmissionPolicy VolumeAttributesClass VolumeGroupSnapshot]  disabled=[ClusterAPIInstall ClusterAPIInstallIBMCloud EventedPLEG GatewayAPI MachineAPIMigration MachineAPIOperatorDisableMachineHealthCheckController MultiArchInstallAzure]
I0120 21:38:12.757925       1 template_controller.go:146] Re-syncing ControllerConfig due to secret pull-secret change
I0120 21:38:12.759118       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
E0120 21:38:12.759174       1 node_controller.go:488] getting scheduler config failed: cluster scheduler couldn't be found
E0120 21:38:12.773539       1 node_controller.go:488] getting scheduler config failed: cluster scheduler couldn't be found
E0120 21:38:12.773574       1 node_controller.go:488] getting scheduler config failed: cluster scheduler couldn't be found
I0120 21:38:12.774066       1 reflector.go:368] Caches populated for *v1.MachineConfigPool from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.800616       1 reflector.go:368] Caches populated for *v1.Node from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.801052       1 reflector.go:368] Caches populated for *v1alpha1.PinnedImageSet from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.802677       1 drain_controller.go:169] Starting MachineConfigController-DrainController
I0120 21:38:12.813391       1 container_runtime_config_controller.go:237] addded image policy observers with sigstore featuregate enabled
I0120 21:38:12.824358       1 reflector.go:368] Caches populated for *v1alpha1.ImageContentSourcePolicy from github.com/openshift/client-go/operator/informers/externalversions/factory.go:125
I0120 21:38:12.832781       1 reflector.go:368] Caches populated for *v1.Infrastructure from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.834943       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go/informers/factory.go:160
I0120 21:38:12.835151       1 machine_set_boot_image_controller.go:221] configMap coreos-bootimages added, reconciling enrolled machine resources
I0120 21:38:12.835213       1 machine_set_boot_image_controller.go:334] MachineConfiguration knobs was not found, so no MAPI machinesets will be enqueued.
I0120 21:38:12.835402       1 reflector.go:368] Caches populated for *v1.ImageDigestMirrorSet from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.842362       1 reflector.go:368] Caches populated for *v1.Scheduler from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.854706       1 reflector.go:368] Caches populated for *v1.MachineConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.863722       1 reflector.go:368] Caches populated for *v1.Image from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.873240       1 pinned_image_set.go:118] Starting MachineConfigController-PinnedImageSetController
I0120 21:38:12.902756       1 reflector.go:368] Caches populated for *v1.ContainerRuntimeConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.906648       1 reflector.go:368] Caches populated for *v1.APIServer from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.906941       1 kubelet_config_controller.go:222] Re-syncing all kubelet config controller generated MachineConfigs due to apiServer cluster change
I0120 21:38:12.907202       1 template_controller.go:198] Re-syncing ControllerConfig due to apiServer cluster change
I0120 21:38:12.915397       1 node_controller.go:248] Starting MachineConfigController-NodeController
I0120 21:38:12.915506       1 render_controller.go:129] Starting MachineConfigController-RenderController
I0120 21:38:12.922506       1 reflector.go:368] Caches populated for *v1.ImageTagMirrorSet from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:12.925744       1 template_controller.go:294] Starting MachineConfigController-TemplateController
I0120 21:38:12.941036       1 reflector.go:368] Caches populated for *v1.KubeletConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:12.970646       1 reflector.go:368] Caches populated for *v1.Pod from k8s.io/client-go/informers/factory.go:160
I0120 21:38:13.000036       1 reflector.go:368] Caches populated for *v1.MachineConfiguration from github.com/openshift/client-go/operator/informers/externalversions/factory.go:125
I0120 21:38:13.000199       1 machine_set_boot_image_controller.go:275] Bootimages management configuration has been added, reconciling enrolled machine resources
I0120 21:38:13.000265       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.000297       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.017044       1 kubelet_config_controller.go:201] Starting MachineConfigController-KubeletConfigController
I0120 21:38:13.028686       1 reflector.go:368] Caches populated for *v1beta1.MachineSet from github.com/openshift/client-go/machine/informers/externalversions/factory.go:125
I0120 21:38:13.029872       1 machine_set_boot_image_controller.go:171] MAPI MachineSet dkhater-01-20-1-ocp-a-srprh-worker-us-east-1a added, reconciling enrolled machine resources
I0120 21:38:13.036143       1 machine_set_boot_image_controller.go:171] MAPI MachineSet dkhater-01-20-1-ocp-a-srprh-worker-us-east-1b added, reconciling enrolled machine resources
I0120 21:38:13.036181       1 machine_set_boot_image_controller.go:171] MAPI MachineSet dkhater-01-20-1-ocp-a-srprh-worker-us-east-1c added, reconciling enrolled machine resources
I0120 21:38:13.036212       1 machine_set_boot_image_controller.go:171] MAPI MachineSet dkhater-01-20-1-ocp-a-srprh-worker-us-east-1d added, reconciling enrolled machine resources
I0120 21:38:13.036244       1 machine_set_boot_image_controller.go:171] MAPI MachineSet dkhater-01-20-1-ocp-a-srprh-worker-us-east-1f added, reconciling enrolled machine resources
I0120 21:38:13.083423       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.083987       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.127793       1 machine_set_boot_image_controller.go:161] Starting MachineConfigController-MachineSetBootImageController
I0120 21:38:13.128303       1 reflector.go:368] Caches populated for *v1alpha1.ClusterImagePolicy from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:13.128923       1 reflector.go:368] Caches populated for *v1alpha1.ImagePolicy from github.com/openshift/client-go/config/informers/externalversions/factory.go:125
I0120 21:38:13.138854       1 reflector.go:368] Caches populated for *v1alpha1.MachineOSConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:13.210353       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.210384       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.229388       1 container_runtime_config_controller.go:247] Starting MachineConfigController-ContainerRuntimeConfigController
I0120 21:38:13.249649       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.249684       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.325192       1 kubelet_config_features.go:126] Applied FeatureSet cluster on MachineConfigPool master
I0120 21:38:13.367340       1 kubelet_config_nodes.go:169] Applied Node configuration 97-worker-generated-kubelet on MachineConfigPool worker
I0120 21:38:13.426849       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.426944       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.468518       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool master
I0120 21:38:13.485708       1 kubelet_config_features.go:126] Applied FeatureSet cluster on MachineConfigPool worker
I0120 21:38:13.617050       1 container_runtime_config_controller.go:957] Applied ImageConfig cluster on MachineConfigPool worker
I0120 21:38:13.806012       1 helpers.go:76] No machine manager were found, so no MAPI machinesets will be enqueued.
I0120 21:38:13.806038       1 machine_set_boot_image_controller.go:361] No MAPI machinesets were enrolled, so no MAPI machinesets will be enqueued.
I0120 21:38:13.824138       1 kubelet_config_nodes.go:169] Applied Node configuration 97-master-generated-kubelet on MachineConfigPool master
I0120 21:38:14.183073       1 reflector.go:368] Caches populated for *v1.ContainerRuntimeConfig from github.com/openshift/client-go/machineconfiguration/informers/externalversions/factory.go:125
I0120 21:38:14.424770       1 kubelet_config_features.go:126] Applied FeatureSet cluster on MachineConfigPool master
I0120 21:38:15.024864       1 kubelet_config_features.go:126] Applied FeatureSet cluster on MachineConfigPool worker
I0120 21:38:17.784373       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:17.784410       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784421       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784427       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784432       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784436       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784440       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784444       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784448       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784452       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784457       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:38:17.784463       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:38:17.784467       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:38:17.784473       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-46-217.ec2.internal
I0120 21:38:17.784478       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-5-10.ec2.internal
I0120 21:38:17.784484       1 node_controller.go:1261] Already picked 2 nodes, capacity is 2, stopping
I0120 21:38:17.784490       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:38:17.784500       1 node_controller.go:1076] worker: 2 candidate nodes in 2 zones for update, capacity: 2
I0120 21:38:17.784537       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:17.784545       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784549       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784553       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784557       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784561       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784564       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784568       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784572       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.784575       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784579       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:38:17.784584       1 node_controller.go:1378] Selected node ip-10-0-46-217.ec2.internal for update (current selection count: 1) [updateCandidateMachines]
I0120 21:38:17.784590       1 node_controller.go:1378] Selected node ip-10-0-5-10.ec2.internal for update (current selection count: 2) [updateCandidateMachines]
I0120 21:38:17.784594       1 node_controller.go:1382] Final list of nodes to update in pool worker: 2 nodes (capacity: 2) [updateCandidateMachines]
I0120 21:38:17.784877       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:17.784891       1 status.go:500] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784896       1 status.go:502] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784900       1 status.go:542] Node ip-10-0-12-37.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784904       1 status.go:500] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784908       1 status.go:502] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784911       1 status.go:542] Node ip-10-0-49-30.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784915       1 status.go:500] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784919       1 status.go:502] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.784922       1 status.go:542] Node ip-10-0-74-240.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.784926       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:38:17.784929       1 node_controller.go:1248] maxUnavailable is 1 and unavail is 0 (getAllCandidateMachines)
I0120 21:38:17.784933       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:38:17.784940       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:38:17.801852       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-46-217.ec2.internal, pool=worker, layered=false, mosbIsNil=true
I0120 21:38:17.823860       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: lost annotation machineconfiguration.openshift.io/desiredImage
I0120 21:38:17.833296       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-5-10.ec2.internal, pool=worker, layered=false, mosbIsNil=true
I0120 21:38:17.850879       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"216260", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Set target for 2 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.852509       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: lost annotation machineconfiguration.openshift.io/desiredImage
I0120 21:38:17.858509       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:38:17.883311       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:17.883342       1 status.go:500] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883349       1 status.go:502] [isNodeUnavailable] Node ip-10-0-12-37.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883354       1 status.go:542] Node ip-10-0-12-37.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.883358       1 status.go:500] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883363       1 status.go:502] [isNodeUnavailable] Node ip-10-0-49-30.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883367       1 status.go:542] Node ip-10-0-49-30.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.883374       1 status.go:500] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal currentConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883379       1 status.go:502] [isNodeUnavailable] Node ip-10-0-74-240.ec2.internal desiredConfig: rendered-master-6a154b7cddf64d36429e9c0f4c5482a3
I0120 21:38:17.883382       1 status.go:542] Node ip-10-0-74-240.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.883386       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:38:17.983012       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:17.983039       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983047       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983052       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.983056       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983060       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983064       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.983068       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983073       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:17.983076       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:38:17.983080       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:38:19.066718       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:38:19.132791       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:38:22.851385       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:38:22.888966       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:22.888995       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:38:22.889001       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:22.889007       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:22.889012       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:22.889016       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:22.889021       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:38:22.889024       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:22.889028       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:38:22.890705       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:38:22.931179       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:22.931201       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:22.931208       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:22.931213       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:22.931217       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:38:22.931221       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:22.931226       1 status.go:495] Node ip-10-0-5-10.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:38:22.931229       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:22.931233       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:38:24.170838       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: cordoning
I0120 21:38:24.170876       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:38:24.192790       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:38:24.213168       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:38:24.221993       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating drain
I0120 21:38:24.238119       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: cordoning
I0120 21:38:24.238149       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:38:24.266297       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:38:24.292895       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:38:24.301319       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating drain
E0120 21:38:26.085540       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-w2bqn, openshift-cluster-node-tuning-operator/tuned-r9wt5, openshift-dns/dns-default-kh6dw, openshift-dns/node-resolver-sxwkk, openshift-image-registry/node-ca-cq7nn, openshift-ingress-canary/ingress-canary-mhb88, openshift-insights/insights-runtime-extractor-sd5zx, openshift-machine-config-operator/machine-config-daemon-m7dpr, openshift-monitoring/node-exporter-tn6kd, openshift-multus/multus-additional-cni-plugins-hrr9q, openshift-multus/multus-pc2gv, openshift-multus/network-metrics-daemon-v7nr4, openshift-network-diagnostics/network-check-target-drltj, openshift-network-operator/iptables-alerter-f2gl5, openshift-ovn-kubernetes/ovnkube-node-f9gcr
I0120 21:38:26.087479       1 drain_controller.go:153] evicting pod openshift-network-diagnostics/network-check-source-77c8b75fd4-lwn9w
I0120 21:38:26.087505       1 drain_controller.go:153] evicting pod openshift-monitoring/openshift-state-metrics-798f7c49-7kwxm
I0120 21:38:26.087519       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
I0120 21:38:26.087508       1 drain_controller.go:153] evicting pod openshift-monitoring/telemeter-client-55469dfcdf-qql6l
I0120 21:38:26.087548       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-ncbp6
I0120 21:38:26.087488       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-fcq79
I0120 21:38:26.087610       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-qjlxt
I0120 21:38:26.087635       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-b6bzw
I0120 21:38:26.087580       1 drain_controller.go:153] evicting pod openshift-monitoring/kube-state-metrics-7789cb954-g7p72
I0120 21:38:26.087587       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-1
I0120 21:38:26.087593       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-ffr42
I0120 21:38:26.087601       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-j6pjl
I0120 21:38:26.087479       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-dv4cd
I0120 21:38:27.556205       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-b6bzw
I0120 21:38:27.856295       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:27.856319       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:27.856325       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:27.856329       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:27.856334       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:38:27.856338       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:27.856342       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:38:27.856345       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:27.856349       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:38:27.957154       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:38:27.957181       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:38:27.957187       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:27.957194       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:38:27.957197       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:38:27.957202       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:27.957208       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:38:27.957212       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:38:27.957215       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
E0120 21:38:28.286510       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-vl8gn, openshift-cluster-node-tuning-operator/tuned-qslsp, openshift-dns/dns-default-vvzxt, openshift-dns/node-resolver-5srtq, openshift-image-registry/node-ca-9psxx, openshift-ingress-canary/ingress-canary-l6pth, openshift-insights/insights-runtime-extractor-hl5q4, openshift-machine-config-operator/machine-config-daemon-dtlln, openshift-monitoring/node-exporter-vb9lg, openshift-multus/multus-additional-cni-plugins-xc5b9, openshift-multus/multus-mp8bq, openshift-multus/network-metrics-daemon-c8vng, openshift-network-diagnostics/network-check-target-vmvjd, openshift-network-operator/iptables-alerter-zhxcz, openshift-ovn-kubernetes/ovnkube-node-qc66q
I0120 21:38:28.288417       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-8dvq7
I0120 21:38:28.288437       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-q2pc4
I0120 21:38:28.288495       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:28.288573       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:38:28.288423       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-s8hr2
I0120 21:38:28.288692       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-9lzxz
I0120 21:38:28.288792       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:28.288971       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-ctn4g
I0120 21:38:28.288427       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-qd4zv
E0120 21:38:28.300708       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-ctn4g" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:28.300775       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:28.300884       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:28.302331       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:28.302709       1 drain_controller.go:153] error when evicting pods/"router-default-787b98474-s8hr2" -n "openshift-ingress" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:28.310638       1 drain_controller.go:153] error when evicting pods/"thanos-querier-68d8f8f746-9lzxz" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:28.341260       1 request.go:700] Waited for 1.130479168s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0
I0120 21:38:28.549234       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-ncbp6
I0120 21:38:29.351529       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/openshift-state-metrics-798f7c49-7kwxm
I0120 21:38:29.541343       1 request.go:700] Waited for 1.386597282s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-diagnostics/pods/network-check-source-77c8b75fd4-lwn9w
I0120 21:38:29.546682       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-network-diagnostics/network-check-source-77c8b75fd4-lwn9w
I0120 21:38:29.748000       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/kube-state-metrics-7789cb954-g7p72
I0120 21:38:30.148052       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/telemeter-client-55469dfcdf-qql6l
I0120 21:38:30.350011       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-1
I0120 21:38:30.741400       1 request.go:700] Waited for 2.424691808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-console/pods/networking-console-plugin-5f88d5854c-8dvq7
I0120 21:38:30.745766       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-8dvq7
I0120 21:38:30.945779       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-q2pc4
I0120 21:38:31.146078       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-qd4zv
I0120 21:38:31.346977       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-0
I0120 21:38:31.546073       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-ffr42
I0120 21:38:31.741510       1 request.go:700] Waited for 2.250261804s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/pods/router-default-787b98474-dv4cd
I0120 21:38:33.301347       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:33.301352       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:33.301927       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-ctn4g
I0120 21:38:33.302482       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:38:33.302784       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-s8hr2
I0120 21:38:33.311531       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-9lzxz
E0120 21:38:33.311636       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-ctn4g" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:33.311848       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:33.312513       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:33.312651       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:35.344701       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-9lzxz
I0120 21:38:38.313000       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:38.313030       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:38.313005       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-ctn4g
I0120 21:38:38.313014       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:38:38.322795       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-ctn4g" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:38.322869       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:38.323189       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:38.323247       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:43.323789       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:38:43.323806       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:43.323815       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-ctn4g
I0120 21:38:43.323798       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:38:43.332410       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:43.332550       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:43.333171       1 drain_controller.go:153] error when evicting pods/"image-registry-c66456f44-ctn4g" -n "openshift-image-registry" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:43.333177       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:48.333176       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
I0120 21:38:48.333195       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:48.333233       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:48.333240       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-ctn4g
E0120 21:38:48.342239       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:48.343327       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:48.344758       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:51.741073       1 request.go:700] Waited for 1.079398986s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/metrics-server-69d8d5dd44-qjlxt
I0120 21:38:53.343372       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:53.343812       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:53.345646       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:38:53.357545       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:53.357929       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:53.358909       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:53.944916       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-fcq79
I0120 21:38:58.358166       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:38:58.358166       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:38:58.359471       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:38:58.373277       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:58.373276       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:38:58.373330       1 drain_controller.go:153] error when evicting pods/"alertmanager-main-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:38:59.945958       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-j6pjl
I0120 21:39:03.373619       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:39:03.373628       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:39:03.373929       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-0
E0120 21:39:03.383188       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:39:03.383962       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:06.425771       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-0
I0120 21:39:08.384065       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:39:08.384391       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
E0120 21:39:08.400112       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
E0120 21:39:08.400129       1 drain_controller.go:153] error when evicting pods/"metrics-server-69d8d5dd44-dt2xd" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:13.400702       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:39:13.400715       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:39:13.410500       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:15.952018       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-ctn4g
I0120 21:39:18.410628       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:39:18.419849       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:23.420511       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:39:23.432022       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:28.432558       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
E0120 21:39:28.444585       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-1" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:39:33.446026       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-1
I0120 21:39:35.500936       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-1
I0120 21:39:43.495410       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-dv4cd
I0120 21:39:50.554791       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-s8hr2
I0120 21:39:56.666974       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-qjlxt" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s
I0120 21:39:58.443456       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-dt2xd" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s
I0120 21:40:56.667152       1 drain_controller.go:380] Previous node drain found. Drain has been going on for 0.04235397935527778 hours
I0120 21:40:56.667180       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating drain
E0120 21:40:57.711420       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-w2bqn, openshift-cluster-node-tuning-operator/tuned-r9wt5, openshift-dns/dns-default-kh6dw, openshift-dns/node-resolver-sxwkk, openshift-image-registry/node-ca-cq7nn, openshift-ingress-canary/ingress-canary-mhb88, openshift-insights/insights-runtime-extractor-sd5zx, openshift-machine-config-operator/machine-config-daemon-m7dpr, openshift-monitoring/node-exporter-tn6kd, openshift-multus/multus-additional-cni-plugins-hrr9q, openshift-multus/multus-pc2gv, openshift-multus/network-metrics-daemon-v7nr4, openshift-network-diagnostics/network-check-target-drltj, openshift-network-operator/iptables-alerter-f2gl5, openshift-ovn-kubernetes/ovnkube-node-f9gcr
I0120 21:40:57.713351       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-qjlxt
I0120 21:40:57.736260       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: operation successful; applying completion annotation
I0120 21:40:58.444494       1 drain_controller.go:380] Previous node drain found. Drain has been going on for 0.042827268506944446 hours
I0120 21:40:58.444519       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating drain
E0120 21:41:00.712831       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-vl8gn, openshift-cluster-node-tuning-operator/tuned-qslsp, openshift-dns/dns-default-vvzxt, openshift-dns/node-resolver-5srtq, openshift-image-registry/node-ca-9psxx, openshift-ingress-canary/ingress-canary-l6pth, openshift-insights/insights-runtime-extractor-hl5q4, openshift-machine-config-operator/machine-config-daemon-dtlln, openshift-monitoring/node-exporter-vb9lg, openshift-multus/multus-additional-cni-plugins-xc5b9, openshift-multus/multus-mp8bq, openshift-multus/network-metrics-daemon-c8vng, openshift-network-diagnostics/network-check-target-vmvjd, openshift-network-operator/iptables-alerter-zhxcz, openshift-ovn-kubernetes/ovnkube-node-qc66q
I0120 21:41:00.714584       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:41:44.735103       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: Evicted pod openshift-monitoring/metrics-server-69d8d5dd44-dt2xd
I0120 21:41:44.754512       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: operation successful; applying completion annotation
I0120 21:42:17.279223       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting OutOfDisk=Unknown
I0120 21:42:17.317181       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:22.285070       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:22.285097       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:22.285103       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:22.285109       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:22.285112       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:22.285118       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:22.285122       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:22.285126       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:22.285130       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:42:22.306048       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:22.306071       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:22.306076       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:22.306081       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:22.306084       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:22.306089       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:22.306094       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:22.306098       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:22.306102       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:42:22.955720       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:27.961111       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:27.961136       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:27.961145       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:27.961150       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:27.961157       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:27.961161       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:27.961167       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:27.961170       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:27.961174       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:42:27.979996       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:27.980023       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:27.980031       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:27.980036       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:27.980039       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:27.980044       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:27.980049       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:27.980053       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:27.980057       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:42:52.460449       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting NotReady=False
I0120 21:42:52.483973       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:52.513381       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:52.869025       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:52.891549       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:42:57.468265       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:57.468289       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:57.468296       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:57.468300       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:57.468308       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:57.468313       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:57.468318       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:57.468322       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:57.468326       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:42:57.488411       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:42:57.488438       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:42:57.488443       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:57.488449       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:57.488453       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:42:57.488457       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:42:57.488463       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:42:57.488467       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:42:57.488471       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:43:04.929093       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: Reporting unready: node ip-10-0-46-217.ec2.internal is reporting Unschedulable
I0120 21:43:04.968152       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:43:07.915986       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting OutOfDisk=Unknown
I0120 21:43:07.916071       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:43:07.952841       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:09.938147       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:09.938173       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:43:09.938178       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:09.938187       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:09.938190       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:09.938196       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:09.938201       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:09.938206       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:09.938210       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:43:09.955795       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:09.955817       1 status.go:490] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal is NOT ready => unavailable
I0120 21:43:09.955822       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:09.955830       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:09.955833       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:09.955839       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:09.955843       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:09.955847       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:09.955853       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:43:13.583646       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:15.660512       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: uncordoning
I0120 21:43:15.660562       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: initiating uncordon (currently schedulable: false)
I0120 21:43:15.691370       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: uncordon succeeded (currently schedulable: true)
I0120 21:43:15.706995       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed taints
I0120 21:43:15.719795       1 drain_controller.go:183] node ip-10-0-46-217.ec2.internal: operation successful; applying completion annotation
I0120 21:43:18.587660       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:18.587685       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:18.587690       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:18.587696       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:18.587701       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:18.587705       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:18.587710       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:43:18.587713       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:18.587717       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:43:18.607752       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:18.607774       1 status.go:495] Node ip-10-0-46-217.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:43:18.607779       1 status.go:539] Node ip-10-0-46-217.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:18.607787       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:18.607790       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:18.607795       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:18.607800       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:18.607804       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:18.607807       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 2
I0120 21:43:20.688731       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Done
I0120 21:43:20.688752       1 node_controller.go:559] Pool worker[zone=us-east-1b]: node ip-10-0-46-217.ec2.internal: lost annotation machineconfiguration.openshift.io/currentImage
I0120 21:43:25.693580       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:25.693604       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693610       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693615       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.693619       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693623       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693628       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.693634       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:25.693638       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:25.693642       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:25.693648       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:43:25.693653       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:43:25.693659       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:43:25.693665       1 node_controller.go:1261] Already picked 1 nodes, capacity is 1, stopping
I0120 21:43:25.693671       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:43:25.693681       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:43:25.693715       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:25.693725       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693730       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693734       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.693739       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:25.693743       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:25.693747       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693751       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.693754       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.693758       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:25.693763       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:43:25.693767       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:43:25.697449       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"217172", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.705042       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:43:25.794137       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:25.794161       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.794167       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.794174       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.794178       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.794182       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:25.794186       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:25.794193       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:25.794197       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:25.794201       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:30.810055       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:30.810081       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810087       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810092       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.810099       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:30.810103       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:30.810108       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810112       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810116       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.810120       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:30.810125       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:43:30.810129       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:43:30.810136       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:43:30.810142       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:43:30.810369       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:43:30.810479       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:30.810516       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810545       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810577       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.810604       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810634       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.810673       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.810706       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:30.810722       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:30.810728       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:30.810734       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:43:30.810739       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:43:30.822035       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"221346", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.829131       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:43:30.910893       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:30.910917       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.910923       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.910928       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.910932       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.910936       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:30.910939       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:30.910946       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:30.910950       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:30.910955       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:39.400769       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting NotReady=False
I0120 21:43:39.442563       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:39.497190       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:43.518133       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:43.548189       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:44.405630       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:44.405655       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405662       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405667       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.405672       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405678       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405682       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.405688       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:44.405692       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:44.405697       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:44.405701       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:43:44.405705       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:43:44.405711       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:43:44.405717       1 node_controller.go:1261] Already picked 1 nodes, capacity is 1, stopping
I0120 21:43:44.405723       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:43:44.405733       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:43:44.405766       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:44.405776       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:44.405780       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:44.405785       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405789       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405794       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.405798       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405802       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.405806       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.405810       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:44.405824       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:43:44.405829       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:43:44.409228       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"221346", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.415393       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:43:44.506320       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:44.506342       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.506350       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.506354       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.506358       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.506363       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:44.506366       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:44.506373       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:44.506376       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:44.506380       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:53.799272       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: Reporting unready: node ip-10-0-5-10.ec2.internal is reporting Unschedulable
I0120 21:43:53.842396       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:58.568945       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:43:58.804170       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:58.804193       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804199       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804204       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.804209       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804213       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804217       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.804221       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:58.804224       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:58.804229       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:58.804234       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:43:58.804238       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:43:58.804243       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:43:58.804250       1 node_controller.go:1261] Already picked 1 nodes, capacity is 1, stopping
I0120 21:43:58.804255       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:43:58.804266       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 1
I0120 21:43:58.804302       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:58.804310       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804315       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804320       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.804323       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:58.804327       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:58.804333       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804337       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.804341       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.804344       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:43:58.804349       1 node_controller.go:1374] Reached capacity limit (1 nodes), stopping selection [updateCandidateMachines]
I0120 21:43:58.804354       1 node_controller.go:1382] Final list of nodes to update in pool worker: 0 nodes (capacity: 1) [updateCandidateMachines]
I0120 21:43:58.807791       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"221346", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Set target for 0 nodes to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.813335       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:43:58.905551       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:43:58.905573       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.905579       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.905584       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.905588       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.905592       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:43:58.905596       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:43:58.905601       1 status.go:490] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal is NOT ready => unavailable
I0120 21:43:58.905605       1 status.go:539] Node ip-10-0-5-10.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:43:58.905609       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:44:03.215781       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: uncordoning
I0120 21:44:03.215817       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: initiating uncordon (currently schedulable: false)
I0120 21:44:03.245379       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: uncordon succeeded (currently schedulable: true)
I0120 21:44:03.260298       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed taints
I0120 21:44:03.272636       1 drain_controller.go:183] node ip-10-0-5-10.ec2.internal: operation successful; applying completion annotation
I0120 21:44:08.240112       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Done
I0120 21:44:08.240680       1 node_controller.go:559] Pool worker[zone=us-east-1a]: node ip-10-0-5-10.ec2.internal: lost annotation machineconfiguration.openshift.io/currentImage
I0120 21:44:08.267565       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:08.267590       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267597       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267601       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267606       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267611       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267614       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267619       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267622       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267626       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267629       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:44:08.267634       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 0 (getAllCandidateMachines)
I0120 21:44:08.267638       1 node_controller.go:1251] calculated initialcapacity is 2 (getAllCandidateMachines)
I0120 21:44:08.267644       1 node_controller.go:1284] Pool worker: selected candidate node ip-10-0-86-209.ec2.internal
I0120 21:44:08.267652       1 node_controller.go:1294] calculated capacity after failingThisConfig is 2 (getAllCandidateMachines)
I0120 21:44:08.267662       1 node_controller.go:1076] worker: 1 candidate nodes in 1 zones for update, capacity: 2
I0120 21:44:08.267697       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:08.267754       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267765       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267771       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267775       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267779       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267784       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267788       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267792       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.267795       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.267799       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:44:08.267836       1 node_controller.go:1378] Selected node ip-10-0-86-209.ec2.internal for update (current selection count: 1) [updateCandidateMachines]
I0120 21:44:08.267843       1 node_controller.go:1382] Final list of nodes to update in pool worker: 1 nodes (capacity: 2) [updateCandidateMachines]
I0120 21:44:08.280462       1 node_controller.go:1197] updateCandidateNode: node=ip-10-0-86-209.ec2.internal, pool=worker, layered=false, mosbIsNil=true
I0120 21:44:08.296349       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: lost annotation machineconfiguration.openshift.io/desiredImage
I0120 21:44:08.296538       1 event.go:377] Event(v1.ObjectReference{Kind:"MachineConfigPool", Namespace:"openshift-machine-config-operator", Name:"worker", UID:"99931134-ecd3-4f30-97ca-27ce887f9e8f", APIVersion:"machineconfiguration.openshift.io/v1", ResourceVersion:"221346", FieldPath:""}): type: 'Normal' reason: 'SetDesiredConfig' Targeted node ip-10-0-86-209.ec2.internal to MachineConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.306999       1 node_controller.go:1110] forceReListNodesFromAPIServer: found 3 nodes for pool worker
I0120 21:44:08.371424       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:08.371446       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371453       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371457       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.371462       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371466       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371469       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.371475       1 status.go:500] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371478       1 status.go:502] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:08.371482       1 status.go:542] Node ip-10-0-86-209.ec2.internal is available (getUnavailableMachines)
I0120 21:44:08.371485       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 0
I0120 21:44:09.788400       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed annotation machineconfiguration.openshift.io/state = Working
I0120 21:44:13.326219       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:13.326331       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.326364       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.326397       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:13.326427       1 status.go:495] Node ip-10-0-86-209.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:44:13.326457       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:44:13.326498       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.326526       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.326555       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:13.326585       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:44:13.326616       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:44:13.326646       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:44:13.326682       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:44:13.327007       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:44:13.404747       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:13.404772       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.404779       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.404784       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:13.404788       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.404791       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:13.404796       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:13.404800       1 status.go:495] Node ip-10-0-86-209.ec2.internal is unavailable: node is in MCD state=Working
I0120 21:44:13.404803       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:44:13.404808       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
E0120 21:44:13.527496       1 render_controller.go:445] Error syncing Generated MCFG: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
E0120 21:44:13.534056       1 render_controller.go:467] Error updating MachineConfigPool worker: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
I0120 21:44:13.534079       1 render_controller.go:382] Error syncing machineconfigpool worker: Operation cannot be fulfilled on machineconfigpools.machineconfiguration.openshift.io "worker": the object has been modified; please apply your changes to the latest version and try again
I0120 21:44:14.895217       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: cordoning
I0120 21:44:14.895244       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating cordon (currently schedulable: true)
I0120 21:44:14.932645       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: cordon succeeded (currently schedulable: false)
I0120 21:44:14.932970       1 node_controller.go:559] Pool worker[zone=us-east-1c]: node ip-10-0-86-209.ec2.internal: changed taints
I0120 21:44:14.960201       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: initiating drain
E0120 21:44:16.021745       1 drain_controller.go:153] WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-csi-drivers/aws-ebs-csi-driver-node-8khhk, openshift-cluster-node-tuning-operator/tuned-g7zp9, openshift-dns/dns-default-mk56w, openshift-dns/node-resolver-hwqv4, openshift-image-registry/node-ca-8msd2, openshift-ingress-canary/ingress-canary-5nfvj, openshift-insights/insights-runtime-extractor-tc9l7, openshift-machine-config-operator/machine-config-daemon-4sr5n, openshift-monitoring/node-exporter-f4mcq, openshift-multus/multus-6kxrr, openshift-multus/multus-additional-cni-plugins-97f84, openshift-multus/network-metrics-daemon-cjxq7, openshift-network-diagnostics/network-check-target-h5k4g, openshift-network-operator/iptables-alerter-x2bn5, openshift-ovn-kubernetes/ovnkube-node-9n6w2
I0120 21:44:16.023760       1 drain_controller.go:153] evicting pod openshift-image-registry/image-registry-c66456f44-94wgx
I0120 21:44:16.023776       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-dh229
I0120 21:44:16.023788       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
I0120 21:44:16.023771       1 drain_controller.go:153] evicting pod openshift-monitoring/telemeter-client-55469dfcdf-7w5fr
I0120 21:44:16.023870       1 drain_controller.go:153] evicting pod openshift-ingress/router-default-787b98474-sc2rg
I0120 21:44:16.023759       1 drain_controller.go:153] evicting pod openshift-network-diagnostics/network-check-source-77c8b75fd4-h25tr
I0120 21:44:16.024090       1 drain_controller.go:153] evicting pod openshift-monitoring/alertmanager-main-1
I0120 21:44:16.023764       1 drain_controller.go:153] evicting pod openshift-monitoring/openshift-state-metrics-798f7c49-7x2hq
I0120 21:44:16.024274       1 drain_controller.go:153] evicting pod openshift-monitoring/kube-state-metrics-7789cb954-cr6nx
I0120 21:44:16.023769       1 drain_controller.go:153] evicting pod openshift-monitoring/thanos-querier-68d8f8f746-crmqw
I0120 21:44:16.024314       1 drain_controller.go:153] evicting pod openshift-monitoring/monitoring-plugin-57b46cfd5c-trf7d
I0120 21:44:16.024321       1 drain_controller.go:153] evicting pod openshift-monitoring/metrics-server-69d8d5dd44-xn2km
I0120 21:44:16.024327       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-7lvsm
I0120 21:44:16.024387       1 drain_controller.go:153] evicting pod openshift-network-console/networking-console-plugin-5f88d5854c-h7dhf
E0120 21:44:16.036918       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:44:18.270382       1 request.go:700] Waited for 1.148558396s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-image-registry/pods/image-registry-c66456f44-94wgx
I0120 21:44:18.331574       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:18.331599       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.331607       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.331612       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:18.331617       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.331621       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.331625       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:18.331630       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:44:18.331634       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:44:18.331640       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:44:18.331645       1 node_controller.go:1248] maxUnavailable is 2 and unavail is 1 (getAllCandidateMachines)
I0120 21:44:18.331648       1 node_controller.go:1251] calculated initialcapacity is 1 (getAllCandidateMachines)
I0120 21:44:18.331656       1 node_controller.go:1294] calculated capacity after failingThisConfig is 1 (getAllCandidateMachines)
I0120 21:44:18.431715       1 status.go:536] getUnavailableMachines: checking 3 nodes (layered=false)
I0120 21:44:18.431737       1 status.go:500] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.431745       1 status.go:502] [isNodeUnavailable] Node ip-10-0-46-217.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.431749       1 status.go:542] Node ip-10-0-46-217.ec2.internal is available (getUnavailableMachines)
I0120 21:44:18.431753       1 status.go:500] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal currentConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.431757       1 status.go:502] [isNodeUnavailable] Node ip-10-0-5-10.ec2.internal desiredConfig: rendered-worker-dddd0a5264747386d04285f0fda42fa6
I0120 21:44:18.431760       1 status.go:542] Node ip-10-0-5-10.ec2.internal is available (getUnavailableMachines)
I0120 21:44:18.431766       1 status.go:490] [isNodeUnavailable] Node ip-10-0-86-209.ec2.internal is NOT ready => unavailable
I0120 21:44:18.431769       1 status.go:539] Node ip-10-0-86-209.ec2.internal marked as unavailable (getUnavailableMachines): layered=false
I0120 21:44:18.431773       1 status.go:545] Total unavailable nodes (getUnavailableMachines): 1
I0120 21:44:19.078086       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/prometheus-operator-admission-webhook-5d9668865-7lvsm
I0120 21:44:19.270384       1 request.go:700] Waited for 1.362964969s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-console/pods/networking-console-plugin-5f88d5854c-h7dhf
I0120 21:44:19.274174       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-h7dhf
I0120 21:44:19.474313       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/kube-state-metrics-7789cb954-cr6nx
I0120 21:44:19.680170       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/openshift-state-metrics-798f7c49-7x2hq
I0120 21:44:19.886813       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/telemeter-client-55469dfcdf-7w5fr
I0120 21:44:20.074551       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-diagnostics/network-check-source-77c8b75fd4-h25tr
I0120 21:44:20.270403       1 request.go:700] Waited for 2.15091988s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-network-console/pods/networking-console-plugin-5f88d5854c-dh229
I0120 21:44:20.274723       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-network-console/networking-console-plugin-5f88d5854c-dh229
I0120 21:44:20.693815       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/alertmanager-main-1
I0120 21:44:21.037435       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
E0120 21:44:21.049779       1 drain_controller.go:153] error when evicting pods/"prometheus-k8s-0" -n "openshift-monitoring" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
I0120 21:44:21.074088       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/thanos-querier-68d8f8f746-crmqw
I0120 21:44:21.270515       1 request.go:700] Waited for 1.95529385s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/monitoring-plugin-57b46cfd5c-trf7d
I0120 21:44:21.273927       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/monitoring-plugin-57b46cfd5c-trf7d
I0120 21:44:26.050450       1 drain_controller.go:153] evicting pod openshift-monitoring/prometheus-k8s-0
I0120 21:44:29.095491       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-monitoring/prometheus-k8s-0
I0120 21:44:43.126413       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-image-registry/image-registry-c66456f44-94wgx
I0120 21:45:33.124938       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Evicted pod openshift-ingress/router-default-787b98474-sc2rg
I0120 21:45:46.568943       1 drain_controller.go:183] node ip-10-0-86-209.ec2.internal: Drain failed. Waiting 1 minute then retrying. Error message from drain: error when waiting for pod "metrics-server-69d8d5dd44-xn2km" in namespace "openshift-monitoring" to terminate: global timeout reached: 1m30s


